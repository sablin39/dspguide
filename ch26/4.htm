	 

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<title>Training the Neural Network</title>
<link href="../new/css/default.css" rel="stylesheet" type="text/css" />

<script type='text/javascript' src='../new/js/jquery-1.5.js'></script>
<script type='text/javascript' src='../new/js/jquery.droppy.js'></script>
<link rel="stylesheet" href="../new/css/droppy.css" type="text/css" />
<script type='text/javascript' src='../new/js/jcc.js'></script>
<script>
$(function(){
	if($.browser.msie){
        if($.browser.version.substr(0,1)!="9"){
            $('#greenBox2').corner({tl:{radius:5}, tr:{radius:5}, bl:{radius:5}, br:{radius:5}, antiAlias:true });
        }
    }else{
        $('#greenBox2').corner({tl:{radius:5}, tr:{radius:5}, bl:{radius:3}, br:{radius:5}, antiAlias:true });
    }
});
</script>

<link rel="stylesheet" href="../new/css/jquery.treeview.css" />
<script src="../new/js/jquery.cookie.js" type="text/javascript"></script>
<script src="../new/js/jquery.treeview.js" type="text/javascript"></script>
<script type="text/javascript" src="../new/js/demo.js"></script>

</head>

<body>


<div id="divPage">
	<div id="wrapper" class="not-logged-in">
		
		<!-- Header -->
		
<div id="header">
			<h1>The Scientist and Engineer's Guide to<br />Digital Signal Processing<br /><span class="txtBlue txt26">By Steven W. Smith, Ph.D.</span></h1>
			<div id="menu">
				<ul id='nav' style="margin-left:10px;"><li><a href="../index.html">Home</a></li><li><a href="../pdfbook.htm" class="selected">The Book by Chapters</a></li><li class="drop"><a href="../about.htm">About the Book</a>					
					<ul>						
						<li><a href="../copyrite.htm">Copyright and permissible use</a></li>							
						<li><a href="../whatdsp.htm">What is DSP?</a></li>
						<li><a href="../eightres.htm">8 good reasons for learning DSP</a></li>
						<li><a href="../reviews.htm">Comments by reviewers</a></li>
						<li><a href="../errata.htm">Errata</a></li>			
						<li><a href="http://www.dspguide.com/ch26/download.htm">Free Software and Teaching Aids</a></li>						
						<li><a href="../editions.htm">Differences Between Editions</a></li>
					</ul>
				  </li><li><a href="../swsmith.htm">Steven W. Smith</a></li><li><a href="http://www.dsprelated.com/blogs-1/nf/Steve_Smith.php">Blog</a></li><li><a href="../contact.htm">Contact</a></li>					
				</ul>
				<script type="text/javascript">$(function() {$("#nav").droppy();});</script>
			</div>
		</div>

		
		<!-- Content -->
		
		<!-- -->		
		<div id="columnLeft">			
			
			<div class="box">
				<h2>Book Search</h2>
				<div id="search">
					<form action="http://www.dspguide.com/search.php" method="post">
						<input type="text" name="searchfor" class="txtField" />
						<input type="image" src="../new/images/btn-go.png" name="Submit" value="Submit" class="button" />
						<div class="clear"><!-- --></div>
					</form>
				</div>
			</div>
		
			
			<div class="box">
				<h2>Download this chapter in PDF format</h2>
				<b><a href="../CH26.PDF">Chapter26.pdf</a></b>
				<br />
				<img src="../new/images/adobe-reader.png" alt="" vspace="5" />
			</div>

			<div class="box">
				<h2>Table of contents</h2>
				<ul id="red" class="treeview-red">	 
					<ul style="border-top:1px solid #aeaeeb;"><li style="border-top:1px solid #aeaeeb;"><a href="../ch1.htm">1: The Breadth and Depth of DSP</a><ul><li style="border-top:1px solid #aeaeeb;"><a href="../ch1/1.htm">The Roots of DSP</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch1/2.htm">Telecommunications</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch1/3.htm">Audio Processing</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch1/4.htm" style="color:#b4b4e9;">Echo Location</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch1/5.htm">Image Processing</a></li></ul></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch2.htm">2: Statistics, Probability and Noise</a><ul><li style="border-top:1px solid #aeaeeb;"><a href="../ch2/1.htm">Signal and Graph Terminology</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch2/2.htm">Mean and Standard Deviation</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch2/3.htm">Signal vs. Underlying Process</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch2/4.htm" style="color:#b4b4e9;">The Histogram, Pmf and Pdf</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch2/5.htm">The Normal Distribution</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch2/6.htm">Digital Noise Generation</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch2/7.htm">Precision and Accuracy</a></li></ul></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch3.htm">3: ADC and DAC</a><ul><li style="border-top:1px solid #aeaeeb;"><a href="../ch3/1.htm">Quantization</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch3/2.htm">The Sampling Theorem</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch3/3.htm">Digital-to-Analog Conversion</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch3/4.htm" style="color:#b4b4e9;">Analog Filters for Data Conversion</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch3/5.htm">Selecting The Antialias Filter</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch3/6.htm">Multirate Data Conversion</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch3/7.htm">Single Bit Data Conversion</a></li></ul></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch4.htm">4: DSP Software</a><ul><li style="border-top:1px solid #aeaeeb;"><a href="../ch4/1.htm">Computer Numbers</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch4/2.htm">Fixed Point (Integers)</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch4/3.htm">Floating Point (Real Numbers)</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch4/4.htm" style="color:#b4b4e9;">Number Precision</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch4/5.htm">Execution Speed: Program Language</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch4/6.htm">Execution Speed: Hardware</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch4/7.htm">Execution Speed: Programming Tips</a></li></ul></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch5.htm">5: Linear Systems</a><ul><li style="border-top:1px solid #aeaeeb;"><a href="../ch5/1.htm">Signals and Systems</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch5/2.htm">Requirements for Linearity</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch5/3.htm">Static Linearity and Sinusoidal Fidelity</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch5/4.htm" style="color:#b4b4e9;">Examples of Linear and Nonlinear Systems</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch5/5.htm">Special Properties of Linearity</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch5/6.htm">Superposition: the Foundation of DSP</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch5/7.htm">Common Decompositions</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch5/8.htm">Alternatives to Linearity</a></li></ul></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch6.htm">6: Convolution</a><ul><li style="border-top:1px solid #aeaeeb;"><a href="../ch6/1.htm">The Delta Function and Impulse Response</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch6/2.htm">Convolution</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch6/3.htm">The Input Side Algorithm</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch6/4.htm" style="color:#b4b4e9;">The Output Side Algorithm</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch6/5.htm">The Sum of Weighted Inputs</a></li></ul></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch7.htm">7: Properties of Convolution</a><ul><li style="border-top:1px solid #aeaeeb;"><a href="../ch7/1.htm">Common Impulse Responses</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch7/2.htm">Mathematical Properties</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch7/3.htm">Correlation</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch7/4.htm" style="color:#b4b4e9;">Speed</a></li></ul></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch8.htm">8: The Discrete Fourier Transform</a><ul><li style="border-top:1px solid #aeaeeb;"><a href="../ch8/1.htm">The Family of Fourier Transform</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch8/2.htm">Notation and Format of the Real DFT</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch8/3.htm">The Frequency Domain's Independent Variable</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch8/4.htm" style="color:#b4b4e9;">DFT Basis Functions</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch8/5.htm">Synthesis, Calculating the Inverse DFT</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch8/6.htm">Analysis, Calculating the DFT</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch8/7.htm">Duality</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch8/8.htm">Polar Notation</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch8/9.htm">Polar Nuisances</a></li></ul></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch9.htm">9: Applications of the DFT</a><ul><li style="border-top:1px solid #aeaeeb;"><a href="../ch9/1.htm">Spectral Analysis of Signals</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch9/2.htm">Frequency Response of Systems</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch9/3.htm">Convolution via the Frequency Domain</a></li></ul></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch10.htm">10: Fourier Transform Properties</a><ul><li style="border-top:1px solid #aeaeeb;"><a href="../ch10/1.htm">Linearity of the Fourier Transform</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch10/2.htm">Characteristics of the Phase</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch10/3.htm">Periodic Nature of the DFT</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch10/4.htm" style="color:#b4b4e9;">Compression and Expansion, Multirate methods</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch10/5.htm">Multiplying Signals (Amplitude Modulation)</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch10/6.htm">The Discrete Time Fourier Transform</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch10/7.htm">Parseval's Relation</a></li></ul></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch11.htm">11: Fourier Transform Pairs</a><ul><li style="border-top:1px solid #aeaeeb;"><a href="../ch11/1.htm">Delta Function Pairs</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch11/2.htm">The Sinc Function</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch11/3.htm">Other Transform Pairs</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch11/4.htm" style="color:#b4b4e9;">Gibbs Effect</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch11/5.htm">Harmonics</a></li><li style="border-top:1px solid #aeaeeb;"><a href="http://www.dspguide.com/ch11/6.htm">Chirp Signals</a></li></ul></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch12.htm">12: The Fast Fourier Transform</a><ul><li style="border-top:1px solid #aeaeeb;"><a href="../ch12/1.htm">Real DFT Using the Complex DFT</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch12/2.htm">How the FFT works</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch12/3.htm">FFT Programs</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch12/4.htm" style="color:#b4b4e9;">Speed and Precision Comparisons</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch12/5.htm">Further Speed Increases</a></li></ul></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch13.htm">13: Continuous Signal Processing</a><ul><li style="border-top:1px solid #aeaeeb;"><a href="../ch13/1.htm">The Delta Function</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch13/2.htm">Convolution</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch13/3.htm">The Fourier Transform</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch13/4.htm" style="color:#b4b4e9;">The Fourier Series</a></li></ul></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch14.htm">14: Introduction to Digital Filters</a><ul><li style="border-top:1px solid #aeaeeb;"><a href="../ch14/1.htm">Filter Basics</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch14/2.htm">How Information is Represented in Signals</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch14/3.htm">Time Domain Parameters</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch14/4.htm" style="color:#b4b4e9;">Frequency Domain Parameters</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch14/5.htm">High-Pass, Band-Pass and Band-Reject Filters</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch14/6.htm">Filter Classification</a></li></ul></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch15.htm">15: Moving Average Filters</a><ul><li style="border-top:1px solid #aeaeeb;"><a href="../ch15/1.htm">Implementation by Convolution</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch15/2.htm">Noise Reduction vs. Step Response</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch15/3.htm">Frequency Response</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch15/4.htm" style="color:#b4b4e9;">Relatives of the Moving Average Filter</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch15/5.htm">Recursive Implementation</a></li></ul></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch16.htm">16: Windowed-Sinc Filters</a><ul><li style="border-top:1px solid #aeaeeb;"><a href="../ch16/1.htm">Strategy of the Windowed-Sinc</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch16/2.htm">Designing the Filter</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch16/3.htm">Examples of Windowed-Sinc Filters</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch16/4.htm" style="color:#b4b4e9;">Pushing it to the Limit</a></li></ul></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch17.htm">17: Custom Filters</a><ul><li style="border-top:1px solid #aeaeeb;"><a href="../ch17/1.htm">Arbitrary Frequency Response</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch17/2.htm">Deconvolution</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch17/3.htm">Optimal Filters</a></li></ul></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch18.htm">18: FFT Convolution</a><ul><li style="border-top:1px solid #aeaeeb;"><a href="../ch18/1.htm">The Overlap-Add Method</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch18/2.htm">FFT Convolution</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch18/3.htm">Speed Improvements</a></li></ul></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch19.htm">19: Recursive Filters</a><ul><li style="border-top:1px solid #aeaeeb;"><a href="../ch19/1.htm">The Recursive Method</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch19/2.htm">Single Pole Recursive Filters</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch19/3.htm">Narrow-band Filters</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch19/4.htm" style="color:#b4b4e9;">Phase Response</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch19/5.htm">Using Integers</a></li></ul></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch20.htm">20: Chebyshev Filters</a><ul><li style="border-top:1px solid #aeaeeb;"><a href="../ch20/1.htm">The Chebyshev and Butterworth Responses</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch20/2.htm">Designing the Filter</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch20/3.htm">Step Response Overshoot</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch20/4.htm" style="color:#b4b4e9;">Stability</a></li></ul></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch21.htm">21: Filter Comparison</a><ul><li style="border-top:1px solid #aeaeeb;"><a href="../ch21/1.htm">Match #1: Analog vs. Digital Filters</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch21/2.htm">Match #2: Windowed-Sinc vs. Chebyshev</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch21/3.htm">Match #3: Moving Average vs. Single Pole</a></li></ul></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch22.htm">22: Audio Processing</a><ul><li style="border-top:1px solid #aeaeeb;"><a href="../ch22/1.htm">Human Hearing</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch22/2.htm">Timbre</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch22/3.htm">Sound Quality vs. Data Rate</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch22/4.htm" style="color:#b4b4e9;">High Fidelity Audio</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch22/5.htm">Companding</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch22/6.htm">Speech Synthesis and Recognition</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch22/7.htm">Nonlinear Audio Processing</a></li></ul></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch23.htm">23: Image Formation & Display</a><ul><li style="border-top:1px solid #aeaeeb;"><a href="../ch23/1.htm">Digital Image Structure</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch23/2.htm">Cameras and Eyes</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch23/3.htm">Television Video Signals</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch23/4.htm" style="color:#b4b4e9;">Other Image Acquisition and Display</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch23/5.htm">Brightness and Contrast Adjustments</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch23/6.htm">Grayscale Transforms</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch23/7.htm">Warping</a></li></ul></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch24.htm">24: Linear Image Processing</a><ul><li style="border-top:1px solid #aeaeeb;"><a href="../ch24/1.htm">Convolution</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch24/2.htm">3x3 Edge Modification</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch24/3.htm">Convolution by Separability</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch24/4.htm" style="color:#b4b4e9;">Example of a Large PSF: Illumination Flattening</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch24/5.htm">Fourier Image Analysis</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch24/6.htm">FFT Convolution</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch24/7.htm">A Closer Look at Image Convolution</a></li></ul></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch25.htm">25: Special Imaging Techniques</a><ul><li style="border-top:1px solid #aeaeeb;"><a href="../ch25/1.htm">Spatial Resolution</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch25/2.htm">Sample Spacing and Sampling Aperture</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch25/3.htm">Signal-to-Noise Ratio</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch25/4.htm" style="color:#b4b4e9;">Morphological Image Processing</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch25/5.htm">Computed Tomography</a></li></ul></li><li class="open" style="border-top:1px solid #aeaeeb;"><a href="../ch26.htm" style="color:#b4b4e9;">26: Neural Networks (and more!)</a><ul><li style="border-top:1px solid #aeaeeb;"><a href="1.htm">Target Detection</a></li><li style="border-top:1px solid #aeaeeb;"><a href="2.htm">Neural Network Architecture</a></li><li style="border-top:1px solid #aeaeeb;"><a href="3.htm">Why Does it Work?</a></li><li style="border-top:1px solid #aeaeeb;"><a href="4.htm" style="color:#b4b4e9;">Training the Neural Network</a></li><li style="border-top:1px solid #aeaeeb;"><a href="5.htm">Evaluating the Results</a></li><li style="border-top:1px solid #aeaeeb;"><a href="6.htm">Recursive Filter Design</a></li></ul></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch27.htm">27: Data Compression</a><ul><li style="border-top:1px solid #aeaeeb;"><a href="../ch27/1.htm">Data Compression Strategies</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch27/2.htm">Run-Length Encoding</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch27/3.htm">Huffman Encoding</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch27/4.htm" style="color:#b4b4e9;">Delta Encoding</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch27/5.htm">LZW Compression</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch27/6.htm">JPEG (Transform Compression)</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch27/7.htm">MPEG</a></li></ul></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch28.htm">28: Digital Signal Processors</a><ul><li style="border-top:1px solid #aeaeeb;"><a href="../ch28/1.htm">How DSPs are Different from Other Microprocessors</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch28/2.htm">Circular Buffering</a></li><li style="border-top:1px solid #aeaeeb;"><a href="http://www.dspguide.com/ch28/3.htm">Architecture of the Digital Signal Processor</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch28/4.htm" style="color:#b4b4e9;">Fixed versus Floating Point</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch28/5.htm">C versus Assembly</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch28/6.htm">How Fast are DSPs?</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch28/7.htm">The Digital Signal Processor Market</a></li></ul></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch29.htm">29: Getting Started with DSPs</a><ul><li style="border-top:1px solid #aeaeeb;"><a href="http://www.dspguide.com/ch29/1.htm">The ADSP-2106x family</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch29/2.htm">The SHARC EZ-KIT Lite</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch29/3.htm">Design Example: An FIR Audio Filter</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch29/4.htm" style="color:#b4b4e9;">Analog Measurements on a DSP System</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch29/5.htm">Another Look at Fixed versus Floating Point</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch29/6.htm">Advanced Software Tools</a></li></ul></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch30.htm">30: Complex Numbers</a><ul><li style="border-top:1px solid #aeaeeb;"><a href="../ch30/1.htm">The Complex Number System</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch30/2.htm">Polar Notation</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch30/3.htm">Using Complex Numbers by Substitution</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch30/4.htm" style="color:#b4b4e9;">Complex Representation of Sinusoids</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch30/5.htm">Complex Representation of Systems</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch30/6.htm">Electrical Circuit Analysis</a></li></ul></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch31.htm">31: The Complex Fourier Transform</a><ul><li style="border-top:1px solid #aeaeeb;"><a href="../ch31/1.htm">The Real DFT</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch31/2.htm">Mathematical Equivalence</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch31/3.htm">The Complex DFT</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch31/4.htm" style="color:#b4b4e9;">The Family of Fourier Transforms</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch31/5.htm">Why the Complex Fourier Transform is Used</a></li></ul></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch32.htm">32: The Laplace Transform</a><ul><li style="border-top:1px solid #aeaeeb;"><a href="../ch32/1.htm">The Nature of the s-Domain</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch32/2.htm">Strategy of the Laplace Transform</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch32/3.htm">Analysis of Electric Circuits</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch32/4.htm" style="color:#b4b4e9;">The Importance of Poles and Zeros</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch32/5.htm">Filter Design in the s-Domain</a></li></ul></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch33.htm">33: The z-Transform</a><ul><li style="border-top:1px solid #aeaeeb;"><a href="../ch33/1.htm">The Nature of the z-Domain</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch33/2.htm">Analysis of Recursive Systems</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch33/3.htm">Cascade and Parallel Stages</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch33/4.htm" style="color:#b4b4e9;">Spectral Inversion</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch33/5.htm">Gain Changes</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch33/6.htm">Chebyshev-Butterworth Filter Design</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch33/7.htm">The Best and Worst of DSP</a></li></ul></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch34.htm">34: Explaining Benford's Law</a><ul><li style="border-top:1px solid #aeaeeb;"><a href="../ch34/1.htm">Frank Benford's Discovery</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch34/2.htm">Homomorphic Processing</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch34/3.htm">The Ones Scaling Test</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch34/4.htm" style="color:#b4b4e9;">Writing Benford's Law as a Convolution</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch34/5.htm">Solving in the Frequency Domain</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch34/6.htm">Solving Mystery #1</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch34/7.htm">Solving Mystery #2</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch34/8.htm">More on Following Benford's law</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch34/9.htm">Analysis of the Log-Normal Distribution</a></li><li style="border-top:1px solid #aeaeeb;"><a href="../ch34/10.htm">The Power of Signal Processing</a></li></ul></li>
					</ul>
				</ul>			
			</div>

			<div class="box">
				<h2>How to order your own hardcover copy</h2>
				Wouldn't you rather have a bound book instead of 640 loose pages?<br />
				Your laser printer will thank you!<br />
				<b>Order from <a href="http://www.amazon.com/Scientist-Engineers-Digital-Signal-Processing/dp/0966017633/ref=pd_bxgy_b_img_a">Amazon.com</a>.</b>
			</div>

		
			
		</div>	

		<!-- -->		
		<div id="columnRight">	
		
			<div id="adbox">
				
			
			</div>	 
			
<div class="breadcrumbs"><a href="../ch26.htm">Chapter 26 - Neural Networks (and more!)</a> / Training the Neural Network</div><h2>Chapter 26: Neural Networks (and more!)</h2><div class="subTitle">Training the Neural Network</div><p><div style="text-align: justify"><p>Neural network design can best be explained with an example.  Figure 26-8
shows the problem we will attack, identifying individual letters in an image of
text.  This pattern recognition task has received much attention.  It is easy
enough that many approaches achieve partial success, but difficult enough that
there are no perfect solutions.  Many successful commercial products have been
based on this problem, such as: reading the addresses on letters for postal
routing, document entry into word processors, etc.</p>

<p>The first step in developing a neural network is to create a database of
examples.  For the text recognition problem, this is accomplished by printing
the 26 capital letters: A,B,C,D &#8230; Y,Z, 50 times on a sheet of paper.  Next, these
1300 letters are converted into a digital image by using one of the many
scanning devices available for personal computers.  This large digital image is
then divided into small images of 10&times;10 pixels, each containing a single letter. 
This information is stored as a 1.3 Megabyte database: 1300 images; 100 pixels
per image; 8 bits per pixel.  We will use the first 260 images in this database to
<i>train</i> the neural network (i.e., determine the weights), and the remainder to <i>test</i>
its performance.   The database must also contain a way of identifying the letter
contained in each image.  For instance, an additional byte could be added to
each 10&times;10 image, containing the letter's ASCII code.  In another scheme,  the
position</p>

<div style="text-align: center; margin: 20px;"><img src="../graphics/F_26_8.gif" border="0" alt=""></img></div>

<p>of each 10&times;10 image in the database could indicate what the letter is.  For
example, images 0 to 49 might all be an "A", images 50-99 might all be a "B",
etc. </p>

<p>For this demonstration, the neural network will be designed for an arbitrary
task: determine which of the 10&times;10 images contains a <i>vowel</i>, i.e., A, E, I, O, or
U.   This may not have any practical application, but it does illustrate the ability
of the neural network to learn very abstract pattern recognition problems.  By
including ten examples of each letter in the training set, the network will
(hopefully) learn the key features that distinguish the target from the nontarget
images.</p>

<p>The neural network used in this example is the traditional three-layer, fully
interconnected architecture, as shown in Figs. 26-5 and 26-6.  There are 101
nodes in the input layer (100 pixel values plus a bias node), 10 nodes in the
hidden layer, and 1 node in the output layer.  When a 100 pixel image is applied
to the input of the network, we want the output value to be close to <i>one</i> if a
vowel is present, and near <i>zero</i> if a vowel is not present.  Don't be worried that
the input signal was acquired as a two-dimensional array (10&times;10), while the
input to the neural network is a one-dimensional array.  This is <i>your</i>
understanding of how the pixel values are interrelated; the <i>neural network</i> will
find relationships of its own. </p>

<p>Table 26-2 shows the main program for calculating the neural network weights,
with Table 26-3 containing three subroutines called from the main program. 
The array elements:  X1[1] through X1[100], hold the input layer values.  In
addition, X1[101] always holds a value of 1, providing the input to the bias
node.  The output values from the hidden nodes are contained</p>

<div style="text-align: center; margin: 20px;"><img src="../graphics/T_26_2.gif" border="0" alt=""></img></div>

<p>in the array elements: X2[1] through X2[10].  The variable, X3, contains the
network's output value.   The weights of the hidden layer are contained in the
array, WH[ , ], where the first index identifies the hidden node (1 to 10), and the
second index is the input layer node (1 to 101).  The weights of the output layer
are held in WO[1] to WO[10].  This makes a total of 1020 weight values that
define how the network will operate.  </p>

<p>The first action of the program is to set each weight to an arbitrary initial value
by using a random number generator.  As shown in lines 190 to 240, the hidden
layer weights are assigned initial values between -0.0005 and 0.0005, while the
output layer weights are between -0.5 and 0.5.   These ranges are chosen to be
the same order of magnitude that the final weights <i>must</i> be.  This is based on:
(1) the range of values in the input signal, (2) the number of inputs summed at
each node, and  (3) the range of values over which the sigmoid is active, an
input of about -5 &lt; <i>x</i> &lt; 5, and an output of 0 to 1.  For instance, when 101 inputs
with a typical value of 100 are multiplied by the typical weight value of 0.0002,
the sum of the products is about 2, which is in the active range of the sigmoid's
input.</p>

<p>If we evaluated the performance of the neural network using these random
weights, we would expect it to be the same as random guessing.  The learning
algorithm improves the performance of the network by gradually changing each
weight in the proper direction.  This is called an <span style="font-weight: bold">iterative</span> procedure, and is
controlled in the program by the FOR-NEXT loop in lines 270-400.    Each
iteration makes the weights slightly more efficient at separating the target from
the nontarget examples.  The iteration loop is usually carried out until no further
improvement is being made.  In typical neural networks, this may be anywhere
from ten to ten-thousand iterations, but a few hundred is common.  This
example carries out 800 iterations.</p>

<p>In order for this iterative strategy to work, there must be a <i>single</i> parameter that
describes how well the system is currently performing.  The variable ESUM (for
error sum) serves this function in the program.   The first action inside the
iteration loop is to set ESUM to zero (line 290) so that it can be used as an
accumulator.   At the end of each iteration, the value of ESUM is printed to the
video screen (line 380), so that the operator can insure that progress is being
made.  The value of ESUM will start high, and gradually decrease as the neural
network is trained to recognize the targets.  Figure 26-9 shows examples of how
ESUM decreases as the iterations proceed.</p>

<p>All 260 images in the training set are evaluated during each iteration, as
controlled by the FOR-NEXT loop in lines 310-360.  Subroutine 1000 is used
to retrieve images from the database of examples.  Since this is not something
of particular interest here, we will only describe the parameters passed to and
from this subroutine.  Subroutine 1000 is entered with the parameter,
LETTER%, being between 1 and 260.  Upon return, the input node values,
X1[1] to X1[100], contain the pixel values for the image in the database
corresponding to LETTER%.  The bias node value, X1[101], is always returned
with a constant value of <i>one</i>.  Subroutine 1000 also returns another parameter,
CORRECT.  This contains the desired output value of the network for this
particular letter.  That is, if the letter in the image is a vowel, CORRECT will
be returned with a value of <i>one</i>.  If the letter in the image is not a vowel,
CORRECT will be returned with a value of <i>zero</i>. </p>

<p>After the image being worked on is loaded into X1[1] through X1[100],
subroutine 2000 passes the data through the current neural network to produce
the output node value, X3.   In other words, subroutine 2000 is the same as the
program in Table 26-1, except for a different number of nodes in each layer.  
This subroutine also calculates how well the current network identifies the letter
as a target or a nontarget.  In line 2210, the variable ELET (for error-letter) is
calculated as the difference between the output value actually generated, X3,
and the desired value, CORRECT.   This makes ELET a value between -1 and
1.   All of the 260 values for ELET are combined (line 340) to form ESUM, the
total squared error of the network for the entire training set.  </p>

<p>Line 2220 shows an option that is often included when calculating the error:
assigning a different <i>importance</i> to the errors for targets and nontargets.  For
example, recall the cancer example presented earlier in this chapter,</p>

<div style="text-align: center; margin: 20px;"><img src="../graphics/T_26_3.gif" border="0" alt=""></img></div>

<p>and the consequences of making a false-positive error versus a false-negative
error.  In the present example, we will arbitrarily declare that the error in
detecting a target is <i>five</i> times as bad as the error in detecting a nontarget.  In
effect, this tells the network to do a better job with the targets, even if it hurts
the performance of the nontargets.</p>

<p>Subroutine 3000 is the heart of the neural network strategy, the algorithm for
changing the weights on each iteration.  We will use an analogy to explain the
underlying mathematics.  Consider the predicament of a military paratrooper
dropped behind enemy lines.  He parachutes to the ground in unfamiliar
territory, only to find it is so dark he can't see more than a few feet away.  His
orders are to proceed to the bottom of the nearest valley to begin the remainder
of his mission.  The problem is, without being able to see more than a few feet,
how does he make his way to the valley floor?  Put another way, he needs an
algorithm to adjust his x and y position on the earth's surface in order to
<i>minimize</i> his elevation.  This is analogous to the problem of adjusting the neural
network weights, such that the network's error, ESUM, is minimized.</p>

<p>We will look at two algorithms to solve this problem: <span style="font-weight: bold">evolution</span> and <span style="font-weight: bold">steepest
descent</span>.  In evolution, the paratrooper takes a flying jump in some random
direction.  If the new elevation is <i>higher</i> than the previous, he curses and returns
to his starting location, where he tries again.  If the new elevation is <i>lower</i>, he
feels a measure of success, and repeats the process from the new location.  
Eventually he will reach the bottom of the valley, although in a very inefficient
and haphazard path.  This method is called <i>evolution</i> because it is the same type
of algorithm employed by nature in biological evolution.  Each new generation
of a species has random variations from the previous.  If these differences are
of benefit to the species, they are more likely to be retained and passed to the
<i>next</i> generation.  This is a result of the improvement allowing the animal to
receive more food, escape its enemies, produce more offspring, etc.  If the new
trait is detrimental, the disadvantaged animal becomes lunch for some predator,
and the variation is discarded.  In this sense, each new generation is an iteration
of the evolutionary optimization procedure. </p>

<p>When evolution is used as the training algorithm, each weight in the neural
network is slightly changed by adding the value from a random number
generator.  If the modified weights make a better network (i.e., a lower value for
ESUM), the changes are retained, otherwise they are discarded.  While this
works, it is very slow in <span style="font-weight: bold">converging</span>.  This is the jargon used to describe that
continual improvement is being made toward an optimal solution (the bottom
of the valley).   In simpler terms, the program is going to need days to reach a
solution, rather than minutes or hours.</p>

<p>Fortunately, the <i>steepest descent</i> algorithm is much faster.  This is how the
paratrooper would naturally respond: evaluate which way is <i>downhill</i>, and move
in that direction. Think about the situation this way.  The paratrooper can move
one step to the north, and record the change in elevation.  After returning to his
original position, he can take one step to the east, and</p>

<div style="text-align: center; margin: 20px;"><img src="../graphics/F_26_9.gif" border="0" alt=""></img></div>

<p>record that elevation change.  Using these two values, he can determine which
direction is downhill.  Suppose the paratrooper drops 10 cm when he moves one
step in the northern direction, and drops 20 cm when he moves one step in the
eastern direction.  To travel directly downhill, he needs to move along each axis
an amount proportional to the slope along that axis.  In this case, he might move
north by 10 steps and east by 20 steps.  This moves him down the steepest part
of the slope a distance of &radic;<span style="text-decoration:overline">10<sup>2</sup> + 20<sup>2</sup></span> steps.  Alternatively, he could move
in a straight line to the new location,  22.4 steps along the diagonal.  The key
point is: <i>the steepest descent is achieved by moving along each axis a distance
proportional to the slope along that axis</i>.  </p>

<p>Subroutine 3000 implements this same steepest decent algorithm for the
network weights. Before entering subroutine 3000, one of the example images
has been applied to the input layer, and the information propagated to the
output.  This means that the values for: X1[ ], X2[ ] and X3 are all specified, as
well as the current weight values: WH[ , ] and WO[ ].  In addition, we know the
error the network produces for this particular image, ELET.  The hidden layer
weights are updated in lines 3050 to 3120, while the output layer weights are
modified in lines 3150 to 3190.  <i>This is done by calculating the slope for each
weight, and then changing each weight by an amount proportional to that slope</i>. 
In the paratrooper case, the slope along an axis is found by moving a small
distance along the axis (say, &Delta;<i>x</i>), measuring the change in elevation (say, &Delta;<i>E</i>),
and then dividing the two (&Delta;<i>E</i>/&Delta;<i>x</i>).  The slope of a neural network weight can
be found in this same way: add a small increment to the weight value (&Delta;<i>w</i>), find
the resulting change in the output signal (&Delta;<i>X3</i>), and divide the two (&Delta;<i>X3</i>/&Delta;<i>w</i>). 
Later in this chapter we will look at an example that calculates the slope this
way.  However, in the present example we will use a more efficient method.</p>

<p>Earlier we said that the nonlinearity (the sigmoid) needs to be <i>differentiable</i>. 
Here is where we will use this property.  If we know the slope at each point on
the nonlinearity, we can directly write an equation for the slope of each weight
(&Delta;<i>X3</i>/&Delta;<i>w)</i> without actually having to perturb it.  Consider a specific weight, for
example, WO[1], corresponding to the first input of the output node.  Look at
the structure in Figs. 26-5 and 26-6, and ask:  how will the output (<i>X3</i>) be
affected if this particular weight (<i>w</i>) is changed slightly, but everything else is
kept the same?  The answer is: </p>



<div style="text-align: center; margin: 20px;"><img src="../graphics/E_26_3.gif" border="0" alt=""></img></div>


<p>where SLOPE<sub>O</sub> is the first derivative of the output layer sigmoid, evaluated
where we are operating on its curve.  In other words, SLOPE<sub>O</sub> describes how
much the <i>output</i> of the sigmoid changes in response to a change in the <i>input</i> to
the sigmoid.  From Eq. 26-2,  SLOPE<sub>O</sub> can be calculated from the current output
value of the sigmoid, X3.  This calculation is shown in line  3160.  In line 3170,
the slope for this weight is calculated via Eq. 26-3, and stored in the variable
DX3DW (i.e., &Delta;<i>X3</i>/&Delta;<i>w).</p>

<p>Using a similar analysis, the slope for a weight on the hidden layer, such as
WH[1,1], can be found by:</p>

<div style="text-align: center; margin: 20px;"><img src="../graphics/E_26_4.gif" border="0" alt=""></img></div>

<p>SLOPE<sub>H1</sub> is the first derivative of the hidden layer sigmoid, evaluated where we
are operating on its curve.  The other values, X1[1] and WO[1], are simply
constants that the weight change sees as it makes its way to the output.  In lines
3070 and 3080, the slopes of the sigmoids are calculated using Eq. 26-2.  The
slope of the hidden layer weight, DX3DW is calculated in line 3090 via Eq. 26-4.   </p>

<p>Now that we know the <i>slope</i> of each of the weights, we can look at how each
weight is changed for the next iteration.  The new value for each weight is found
by taking the current weight, and adding an amount that is proportional to the
slope:</p>

<div style="text-align: center; margin: 20px;"><img src="../graphics/E_26_5.gif" border="0" alt=""></img></div>

<p>This calculation is carried out in line 3100 for the hidden layer, and line 3180
for the output layer.  The proportionality constant consists of two factors,
ELET, the error of the network for this particular input, and MU, a constant set
at the beginning of the program.  To understand the need for ELET in this
calculation, imagine that an image placed on the input produces a <i>small</i> error in
the output signal.  Next, imagine that another image applied to the input
produces a <i>large</i> output error.  When adjusting the weights, we want to nudge
the network more for the second image than the first.  If something is working
poorly, we want to change it; if it is working well, we want to leave it alone. 
This is accomplished by changing each weight in proportion to the current error,
ELET. </p>

<p>To understand how MU affects the system, recall the example of the
paratrooper.  Once he determines the downhill direction, he must decide how
far to proceed before reevaluating the slope of the terrain.  By making this
distance short, one meter for example, he will be able to precisely follow the
contours of the terrain and always be moving in an optimal direction.  The
problem is that he spends most of his time evaluating the slope, rather than
actually moving down the hill.   In comparison, he could choose the distance to
be large, say 1000 meters.  While this would allow the paratrooper to move
rapidly along the terrain, he might overshoot the downhill path.  Too large of
a distance makes him jump all over the country-side without making the desired
progress.  </p>

<p>In the neural network, MU controls how much the weights are changed on each
iteration.  The value to use depends on the particular problem, being as low as
10<sup>-6</sup>, or as high as 0.1.  From the analogy of the paratrooper, it can be expected
that too small of a value will cause the network to converge too slowly.  In
comparison, too large of a value will cause the convergence to be erratic, and
will exhibit chaotic oscillation around the final solution.  Unfortunately, the
way neural networks react to various values of MU can be difficult to
understand or predict.  This makes it critical that the network error (i.e., ESUM)
be monitored during the training, such as printing it to the video screen at the
end of each iteration.  If the system isn't converging properly, stop the program
and try another value for MU.</p></div></p>Next Section: <a href="5.htm">Evaluating the Results</a>			

		</div>
		<div class="clear"><!-- --></div>
		

	</div>
</div>

<!-- Footer -->

<div id="footer">
	<a href="../index.html">Home</a> | <a href="../pdfbook.htm">The Book by Chapters</a> | <a href="../about.htm">About the Book</a> | <a href="../swsmith.htm">Steven W. Smith</a> | <a href="http://www.dsprelated.com/blogs-1/nf/Steve_Smith.php">Blog</a> | <a href="../contact.htm">Contact</a>
	<br />
	Copyright © 1997-2011 by California Technical Publishing
</div>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
try {
var pageTracker = _gat._getTracker("UA-1774944-11");
pageTracker._trackPageview();
} catch(err) {}</script>

</body>
</html>
